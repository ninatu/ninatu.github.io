<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Nina Shvetsova</title>
  
  <meta name="author" content="Nina Shvetsova">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Nina Shvetsova</name>
              </p>
<!--              <p>I am a senior staff research scientist at <a href="https://ai.google/research">Google Research</a>, where I work on computer vision and machine learning.-->
                  I am a PhD student at the <a href="http://www.cvai.cs.uni-frankfurt.de/">Computational Vision & Artificial Intelligence</a> group at <a href="https://www.goethe-university-frankfurt.de/en?legacy_request=1"> Goethe University Frankfurt</a>, advised by <a href="https://hildekuehne.github.io/">Prof. Hilde Kuehne</a>.  My primary research areas are deep learning for video and image understanding and multi-modal learning.


              </p>
              <p>
<!--                At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.-->
                    I received M.S. and B.S. degrees in Computer Science at the Moscow State University, where I was working on unsupervised image anomaly detection advised by <a href="https://scholar.google.com/citations?user=ZT_k-wMAAAAJ&hl=en">Prof. Anton Konushin</a>. During my master's, I also worked as a junior scientist in <a href="https://www.philips.com/a-w/about/innovation/research.html">Philips Research</a> advised by <a href="researchgate.net/profile/Bart-Bakker-2/research">Bart Bakker</a>, <a href="https://scholar.google.com/citations?user=c4ZlKeMAAAAJ&hl=en">Irina Fedulova</a>, and <a href="https://scholar.google.com/citations?user=mhhvib8AAAAJ&hl=en">Dmitry V. Dylov</a> on medical image analysis problems, including domain adaption, disease detection, localization, and segmentation problems.
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=qZtU1L4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
<!--                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp-->
                <a href="https://github.com/ninatu/">Github</a>  &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/nina-shvetsova-tuluptceva-424ba1143/">LinkedIn</a>  &nbsp/&nbsp
                <a href="mailto:shvetsov@frankfurt.de">Email</a> &nbsp/&nbsp
                <a href="data/bio/NinaShvetsova-CV.pdf">CV</a>

<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->

              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="data/bio/NinaShvetsova.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="data/bio/NinaShvetsova.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>

<!--                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.-->
                    My current research interest lies in the field of unsupervised and semi-supervised machine learning techniques for video and image understanding, including multi-modal learning utilizing text and audio modalities.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <!--          Everything at Once-Multi-Modal Fusion Transformer for Video Retrieval                 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/shvetsova2022everything2.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf" id="MCG_journal">
                <papertitle>Everything at Once-Multi-Modal Fusion Transformer for Video Retrieval</papertitle>
              </a>
              <br>
              <strong>Nina Shvetsova</strong>,
              <a href="https://brian7685.github.io/">Brian Chen</a>,
              <a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a>,
              <a href="https://scholar.google.com/citations?user=S34WHG0AAAAJ&hl=en">Samuel Thomas</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-bedk">Brian Kingsbury</a>,
              <a href="https://www.rogerioferis.org/">Rogerio S Feris</a>,
              <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>,
              <a href="https://people.csail.mit.edu/jrg/">James Glass</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf">paper</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Shvetsova_Everything_at_Once_CVPR_2022_supplemental.pdf">supplement</a> /
              <a href="https://arxiv.org/abs/2112.04446">arXiv</a> /
              <a href="data/bib/shvetsova2022everything.bib">bibtex</a> /
              <a href="https://github.com/ninatu/everything_at_once">code</a>
              <p></p>
              <p>Modality-agnostic self-attention blocks, trained on everything at once  ‚Äì all combinations of modalities, can produce a fused representation of any number of input modalities.</p>
            </td>
          </tr>

        <!--          MOOD 2020: A public Benchmark for Out-of-Distribution Detection and Localization on medical Images                -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/zimmerer2022mood.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9762702" id="MCG_journal">
                <papertitle>MOOD 2020: A public Benchmark for Out-of-Distribution Detection and Localization on medical Images</papertitle>
              </a>
              <br>
              David Zimmerer, Peter M Full, Fabian Isensee, Paul J√§ger, Tim Adler,
              Jens Petersen, Gregor K√∂hler, Tobias Ross, Annika Reinke, Antanas Kascenas,
              Bj√∏rn Sand Jensen, Alison Q O‚ÄôNeil, Jeremy Tan, Benjamin Hou, James Batten,
              Huaqi Qiu, Bernhard Kainz,
              <strong>Nina Shvetsova</strong>, Irina Fedulova, Dmitry V Dylov,
              Baolun Yu, Jianyang Zhai, Jingtao Hu, Runxuan Si, Sihang Zhou, Siqi Wang, Xinyang Li,
              Xuerun Chen, Yang Zhao, Sergio Naval Marimont, Giacomo Tarroni,
              Victor Saase, Lena Maier-Hein, Klaus Maier-Hein
              <br>
              <em>IEEE Transactions on Medical Imaging</em>, 2022
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9762702">paper</a> /
              <a href="data/bib/zimmerer2022mood.bib">bibtex</a> /
              <a href="https://github.com/ninatu/mood_challenge">code of our solution</a>
              <p></p>
              </td>
          </tr>

        <!--          Routing with Self-Attention for Multimodal Capsule Networks                  -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/duarte2021routing.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.00775" id="MCG_journal">
                <papertitle>Routing with Self-Attention for Multimodal Capsule Networks</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=PxD5DrYAAAAJ&hl=en">Kevin Duarte</a>,
              <a href="https://brian7685.github.io/">Brian Chen</a>,
              <strong>Nina Shvetsova</strong>,
              <a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a>,
              <a href="https://scholar.google.com/citations?user=S34WHG0AAAAJ&hl=en">Samuel Thomas</a>,
              <a href="https://alexander-h-liu.github.io/">Alexander Liu</a>,
              <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>,
              <a href="https://people.csail.mit.edu/jrg/">James Glass</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=p8gsO3gAAAAJ&view_op=list_works&sortby=pubdate">Mubarak Shah</a>,
              <br>
              <em>arxiv</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2112.00775">arXiv</a> /
              <a href="data/bib/duarte2021routing.bib">bibtex</a>
              <p></p>
              <p>Qualities of capsule architectures is used in the context of multimodal learning to learn similar concepts across different modalities.</p>
            </td>
          </tr>

        <!--          Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders                  -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/shvetsova2021anomaly2.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521238" id="MCG_journal">
                <papertitle>Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders</papertitle>
              </a>
              <br>
              <strong>Nina Shvetsova</strong>,
              <a href="https://www.researchgate.net/profile/Bart-Bakker-2">Bart Bakker</a>,
              <a href="https://scholar.google.com/citations?user=c4ZlKeMAAAAJ&hl=en">Irina Fedulova</a>,
              <a href="https://www.researchgate.net/profile/Heinrich-Schulz">Heirich Schulz</a>,
              <a href="https://scholar.google.com/citations?user=mhhvib8AAAAJ&hl=en">Dmitry V. Dylov</a>
              <br>
              <em>IEEE Access</em>, 2021
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521238">paper</a> /
              <a href="https://arxiv.org/abs/2006.13265">arXiv</a> /
              <a href="data/bib/shvetsova2021anomaly.bib">bibtex</a> /
              <a href="https://github.com/ninatu/anomaly_detection">code</a>
              <p></p>
              <p>We establish a strong baseline in anomaly detection in medical images by extending deep autoencoder with progressive growing training to handle high-resolution, complex images.</p>
            </td>
          </tr>

<!--          PIAD                  -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/tuluptceva2019perceptual2.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-41404-7_12" id="MCG_journal">
                <papertitle>Perceptual Image Anomaly Detection</papertitle>
              </a>
              <br>
              <strong>Nina Tuluptceva</strong>,
              <a href="https://www.researchgate.net/profile/Bart-Bakker-2">Bart Bakker</a>,
              <a href="https://scholar.google.com/citations?user=c4ZlKeMAAAAJ&hl=en">Irina Fedulova</a>,
              <a href="https://scholar.google.com/citations?user=ZT_k-wMAAAAJ&hl=en">Anton Konushin</a>
              <br>
              <em>ACPR</em>, 2019
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-41404-7_12">paper</a> /
              <a href="https://arxiv.org/abs/1909.05904">arXiv</a> /
              <a href="data/bib/tuluptceva2019perceptual.bib">bibtex</a> /
              <a href="https://github.com/ninatu/piad">code</a>
              <p></p>
              <p>We present a novel method for image anomaly detection leveraging Generative Adversarial Networks to map an image distribution to a predefined latent distribution and vice versa.</p>
              <p>This paper took IAPR Best Paper Award at <a href="https://www.acpr2019.org/">ACPR‚Äô19</a> </p>
            </td>
          </tr>


<!--&lt;!&ndash;          video example&ndash;&gt;-->
<!--          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>-->
<!--                <source src="images/nerf_supervision.mp4" type="video/mp4">-->
<!--                Your browser does not support the video tag.-->
<!--                </video></div>-->
<!--                <img src='images/nerf_supervision.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function nerfsuper_start() {-->
<!--                  document.getElementById('nerfsuper_image').style.opacity = "1";-->
<!--                }-->

<!--                function nerfsuper_stop() {-->
<!--                  document.getElementById('nerfsuper_image').style.opacity = "0";-->
<!--                }-->
<!--                nerfsuper_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--							<a href="https://waymo.com/research/block-nerf/">-->
<!--                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, -->
<!--              <a href="http://www.peteflorence.com/">Pete Florence</a>, -->
<!--              <strong>Jonathan T. Barron</strong>,  <br>-->
<!--              <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, -->
<!--              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,-->
<!--              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>-->
<!--              <br>-->
<!--              <em>ICRA</em>, 2022  -->
<!--              <br>-->
<!--							<a href="http://yenchenlin.me/nerf-supervision/">project page</a> / -->
<!--							<a href="https://arxiv.org/abs/2203.01913">arXiv</a> / -->
<!--							<a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /-->
<!--							<a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / -->
<!--							<a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				-->
<!--              <p></p>-->
<!--              <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--            &lt;!&ndash;   image example&ndash;&gt;-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/PABMM2015.jpg" alt="PontTuset" width="160" style="border-style: none">-->
<!--            </td>-->
<!--            <td width="75%" valign="middle">-->
<!--              <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal">-->
<!--                <papertitle>Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>-->
<!--              <br>-->
<!--              <em>TPAMI</em>, 2017-->
<!--              <br>-->
<!--              <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /-->
<!--              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /-->
<!--              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a>-->
<!--              <p></p>-->
<!--              <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p>-->
<!--              <p>This paper subsumes our CVPR 2014 paper.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          &lt;!&ndash;   two images example&ndash;&gt;-->
<!--          <tr onmouseout="ddp_stop()" onmouseover="ddp_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='ddp_image'>-->
<!--                  <img src='images/ddp_after.jpg' width="160"></div>-->
<!--                <img src='images/ddp_before.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function ddp_start() {-->
<!--                  document.getElementById('ddp_image').style.opacity = "1";-->
<!--                }-->

<!--                function ddp_stop() {-->
<!--                  document.getElementById('ddp_image').style.opacity = "0";-->
<!--                }-->
<!--                ddp_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/2112.03288">-->
<!--                <papertitle>Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--                    <a href="https://niessnerlab.org/members/barbara_roessle/profile.html">Barbara Roessle</a>,-->
<!--                    <strong>Jonathan T. Barron</strong>,-->
<!--                    <a href="https://bmild.github.io/">Ben Mildenhall</a>,-->
<!--                    <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,-->
<!--                    <a href="https://www.niessnerlab.org/">Matthias Nie√üner</a>-->
<!--              <br>-->
<!--                    <em>CVPR</em>, 2022-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/2112.03288">arXiv</a>-->
<!--              /-->
<!--              <a href="https://www.youtube.com/watch?v=zzkvvdcvksc">video</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Dense depth completion techniques applied to freely-available sparse stereo data can improve NeRF reconstructions in low-data regimes.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
	

<!--        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>-->
<!--          <tr>-->
<!--            <td>-->
<!--              <heading>Misc</heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="20"><tbody>-->
<!--					-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>-->
<!--              <br>-->
<!--              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>-->
<!--              <br>-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/cs188.jpg" alt="cs188">-->
<!--            </td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>-->
<!--              <br>-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>-->
<!--              <br>-->
<!--              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--					-->

<!--          <tr>-->
<!--            <td align="center" style="padding:20px;width:25%;vertical-align:middle">-->
<!--							<heading>Basically <br> Blog Posts</heading>-->
<!--            </td>-->
<!--            <td width="75%" valign="middle">-->
<!--              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>-->
<!--            </td>-->
<!--          </tr>-->
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source code and design are borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>,
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
