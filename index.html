<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Nina Shvetsova</title>
  
  <meta name="author" content="Nina Shvetsova">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Nina Shvetsova</name>
              </p>
                  I am a fourth-year PhD student at <a href="https://www.uni-bonn.de/en">University of Bonn</a> (previously at <a href="http://www.cvai.cs.uni-frankfurt.de/">Goethe University Frankfurt</a>), advised by <a href="https://hildekuehne.github.io/">Prof. Hilde Kuehne</a>, and a visiting PhD student at the <a href="https://www.mpi-inf.mpg.de/home/">Max Planck Institute for Informatics</a>, advised by <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele/">Prof. Bernt Schiele</a>. As part of <a href="https://ellis.eu/phd-postdoc">ELLIS PhD program</a>, I'm also co-supervised by <a href="https://chrirupp.github.io/">Prof. Christian Rupprecht</a>, <a href="https://www.ox.ac.uk/">University of Oxford</a>. I'm also participating in <a href="https://sightandsound.csail.mit.edu/">MIT-IBM Watson Sight and Sound Project</a>.
                  My primary research area is deep learning for video and image understanding through self-supervised and multi-modal learning.
              <p>

              </p>

              <p>
                    Before this, I received B.S. and M.S. degrees in Computer Science at the <a href="https://www.msu.ru/en/">Moscow State University</a>, where I worked on image anomaly detection, advised by <a href="https://scholar.google.com/citations?user=ZT_k-wMAAAAJ&hl=en">Prof. Anton Konushin</a>. During my master's, I also worked in <a href="https://www.philips.com/a-w/about/innovation/research.html">Philips Research</a> on medical image analysis.
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=qZtU1L4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ninatu/">Github</a>  &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/nina-shvetsova-424ba1143/">LinkedIn</a>  &nbsp/&nbsp
                <a href="https://twitter.com/ninashv__">Twitter</a>

              </p>
                            <p style="text-align:center">
                              Email: shvetsov at uni-bonn.de
                              <br>
                              Old email: shvetsov at uni-frankfurt.de
                            </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="data/bio/NinaShvetsova.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="data/bio/NinaShvetsova.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <b>07.2024</b> &emsp;I'm participating in the International Computer Vision Summer School - <a href="https://iplab.dmi.unict.it/icvss2024/Home"><b>ICVSS 2024!</b></a>
              </p>
              <p>
                <b>07.2024</b> &emsp;One paper is accepted to <b>ECCV 2024</b>! Check it out:  <a href="https://arxiv.org/pdf/2310.04900">HowToCaption: Prompting LLMs to Transform Video Annotations at Scale.</a>
              </p>
              <p>
                <b>06.2024</b> &emsp;I will serve as an Area Chair of <b>WACV 2025</b>.
              </p>
              <p>
                <b>02.2024</b> &emsp;One paper is accepted to <b>CVPR 2024</b>! Check it out:  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_What_When_and_Where_Self-Supervised_Spatio-Temporal_Grounding_in_Untrimmed_Multi-Action_CVPR_2024_paper.pdf">What, when, and where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions.</a>
              </p>
              <p>
                <b>01.2024</b> &emsp;I will be attending the <a href="https://www.bmva.org/meetings/24-01-17-Vision%20and%20Language.html">BMVA Symposium on Vision and Language</a> with a poster presenting our recent works.
              </p>
              <p>
                <b>01.2024</b> &emsp;I started my PhD research visit with the <a href="https://www.robots.ox.ac.uk/~vgg/">VGG at Oxford</a> !
              </p>
              <p>
              <p>
                <b>12.2023</b> &emsp;Our workshop on <a href="https://sites.google.com/view/2nd-mmfm-workshop/home">"What is Next in Multimodal Foundation Models"</a> has been accepted at <b>CVPR 2024</b>! <br>
                Check out <a href="https://sites.google.com/view/2nd-mmfm-workshop/call-for-papers">Call For Papers</a>!
              </p>
              <p>
                <b>09.2023</b> &emsp;I will present two of our recent works: "Learning by Sorting: Self-supervised Learning with Group Ordering Constraints" and "In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval" in the Nectar Track at <b>GCPR 2023</b>.
              </p>
              <p>
                <b>08.2023</b> &emsp;I will serve as an Area Chair of <b>WACV 2024</b>.
              </p>
              <p>
                <b>07.2023</b> &emsp;Four papers are accepted to <b>ICCV 2023</b>!
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Featured Research</heading>
              <p>

                    My current research interest lies in the field of self-supervised learning for video and image understanding, including multi-modal learning utilizing text and audio modalities.
              </p>
            </td>
          </tr>
        </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!--         HowToCaption: Prompting LLMs to Transform Video Annotations at Scale              -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/shvetsova2023howtocaption.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2310.04900" id="MCG_journal">
                <papertitle>HowToCaption: Prompting LLMs to Transform Video Annotations at Scale</papertitle>
              </a>
              <br>
              <strong>Nina Shvetsova*</strong>,
              <a href="https://annusha.github.io/">Anna Kukleva*</a>,
              <a href="https://xudonghong.me/">Xudong Hong</a>,
              <a href="https://chrirupp.github.io/">Christian Rupprecht</a>,
              <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>  (*equal contribution)
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2310.04900">arXiv</a> /
              <a href=https://github.com/ninatu/howtocaption>(code coming soon)</a>
              <p></p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!--         What, when, and where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions            -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/chen2024what.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_What_When_and_Where_Self-Supervised_Spatio-Temporal_Grounding_in_Untrimmed_Multi-Action_CVPR_2024_paper.pdf" id="MCG_journal">
                <papertitle>What, when, and where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions </papertitle>
              </a>
              <br>
              <a href="https://brian7685.github.io/">Brian Chen</a>,
              <strong>Nina Shvetsova</strong>,
              <a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a>,
              <a href="https://scholar.google.com/citations?user=4I1TxJcAAAAJ&hl=en&oi=ao">Daniel Kondermann</a>,
              <a href="https://scholar.google.com/citations?user=S34WHG0AAAAJ&hl=en">Samuel Thomas</a>,
              <a href="https://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a>,
              <a href="https://www.rogerioferis.org/">Rogerio S Feris</a>,
              <a href="https://people.csail.mit.edu/jrg/">James Glass</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_What_When_and_Where_Self-Supervised_Spatio-Temporal_Grounding_in_Untrimmed_Multi-Action_CVPR_2024_paper.pdf">paper</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Chen_What_When_and_CVPR_2024_supplemental.pdf">supplement</a> /
              <a href="https://arxiv.org/abs/2303.16990">arXiv</a> /
              <a href=https://github.com/brian7685/STG>code</a>
              <p></p>
            </td>
          </tr>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!--          In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval              -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/shvetsova2023in-style.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2309.08928" id="MCG_journal">
                <papertitle>In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval</papertitle>
              </a>
              <br>
              <strong>Nina Shvetsova*</strong>,
              <a href="https://annusha.github.io/">Anna Kukleva*</a>,
              <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>  (*equal contribution)
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2309.08928">arXiv</a> /
              <a href="data/bib/shvetsova2023in-style.bib">bibtex</a> /
              <a href=https://github.com/ninatu/in_style>(code coming soon)</a>
              <p></p>
            </td>
          </tr>

        <!--          Learning by Sorting: Self-supervised Learning with Group Ordering Constraints               -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/shvetsova2023learning.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2301.02009.pdf" id="MCG_journal">
                <papertitle>Learning by Sorting: Self-supervised Learning with Group Ordering Constraints</papertitle>
              </a>
              <br>
              <strong>Nina Shvetsova</strong>,
              <a href="https://petersen.ai/">Felix Petersen</a>,
              <a href="https://annusha.github.io/">Anna Kukleva</a>,
              <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shvetsova_Learning_by_Sorting_Self-supervised_Learning_with_Group_Ordering_Constraints_ICCV_2023_paper.pdf">paper</a> /
              <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Shvetsova_Learning_by_Sorting_ICCV_2023_supplemental.pdf">supplement</a> /
              <a href="https://arxiv.org/abs/2301.02009">arXiv</a> /
              <a href="data/bib/shvetsova2023learning.bib">bibtex</a> /
              <a href=https://github.com/ninatu/learning_by_sorting>code</a>
              <p></p>
            </td>
          </tr>

        <!--          Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge           -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/lin2023match.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2303.08914" id="MCG_journal">
                <papertitle>Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge</papertitle>
              </a>
              <br>
              <a href="https://wlin-at.github.io/">Wei Lin</a>,
              <a href="https://scholar.google.com/citations?user=WbO7tjYAAAAJ&hl=en">Leonid Karlinsky</a>,
              <strong>Nina Shvetsova</strong>,
              <a href="https://snototter.github.io/research/">Horst Possegger</a>,
              <a href="https://scholar.google.com/citations?user=oDDqnQ4AAAAJ&hl=en">Mateusz Kozinski</a>,
              <a href="https://rpand002.github.io/">Rameswar Panda</a>, <br>
              <a href="https://www.rogerioferis.org/">Rogerio Feris</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <a href="https://www.tugraz.at/institute/icg/research/team-bischof/people/team-about/horst-bischof">Horst Bischof</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_MAtch_eXpand_and_Improve_Unsupervised_Finetuning_for_Zero-Shot_Action_Recognition_ICCV_2023_paper.pdf">paper</a> /
              <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Lin_MAtch_eXpand_and_ICCV_2023_supplemental.pdf">supplement</a> /
              <a href="https://arxiv.org/abs/2303.08914">arxiv</a> /
              <a href="data/bib/lin2023match.bib">bibtex</a> /
              <a href=https://github.com/wlin-at/MAXI>code</a>
              <p></p>
            </td>
          </tr>

        <!--          Preserving Modality Structure Improves Multi-Modal Learning          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/sirnam2023preserving.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2308.13077" id="MCG_journal">
                <papertitle>Preserving Modality Structure Improves Multi-Modal Learning</papertitle>
              </a>
              <br>
              <a href="https://swetha5.github.io/">Sirnam Swetha</a>,
              <a href="https://nayeemrizve.github.io/">Mamshad Nayeem Rizve</a>,
              <strong>Nina Shvetsova</strong>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <a href="http://crcv.ucf.edu/">Mubarak Shah</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Swetha_Preserving_Modality_Structure_Improves_Multi-Modal_Learning_ICCV_2023_paper.pdf">paper</a> /
              <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Swetha_Preserving_Modality_Structure_ICCV_2023_supplemental.pdf">supplement</a> /
              <a href="https://arxiv.org/abs/2308.13077">arxiv</a> /
              <a href="data/bib/sirnam2023preserving.bib">bibtex</a> /
              <a href=https://github.com/Swetha5/Multi_Sinkhorn_Knopp>code</a>
              <p></p>
            </td>
          </tr>

         <!--          C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval            -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/rouditschenko2023c2kd.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/iel7/10094559/10094560/10094821.pdf" id="MCG_journal">
                <papertitle>C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval</papertitle>
              </a>
              <br>
              <a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a>,
              <a href="https://people.csail.mit.edu/yungsung/">Yung-Sung Chuang</a>,
              <strong>Nina Shvetsova</strong>,
              <a href="https://scholar.google.com/citations?user=S34WHG0AAAAJ&hl=en">Samuel Thomas</a>,
              <a href="https://www.rogerioferis.org/">Rogerio Feris</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-bedk">Brian Kingsbury</a>,
              <a href="https://scholar.google.com/citations?user=WbO7tjYAAAAJ&hl=en">Leonid Karlinsky</a>,
              <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <a href="https://people.csail.mit.edu/jrg/">James Glass</a>,
              <br>
              <em>ICASSP </em>, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/iel7/10094559/10094560/10094821.pdf">paper</a> /
              <a href="https://arxiv.org/abs/2210.03625">arXiv</a> /
              <a href="https://github.com/roudimit/c2kd">code</a>
              <p></p>
            </td>
          </tr>


        <!--          Everything at Once-Multi-Modal Fusion Transformer for Video Retrieval                 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/shvetsova2022everything2.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf" id="MCG_journal">
                <papertitle>Everything at Once-Multi-Modal Fusion Transformer for Video Retrieval</papertitle>
              </a>
              <br>
              <strong>Nina Shvetsova</strong>,
              <a href="https://brian7685.github.io/">Brian Chen</a>,
              <a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a>,
              <a href="https://scholar.google.com/citations?user=S34WHG0AAAAJ&hl=en">Samuel Thomas</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-bedk">Brian Kingsbury</a>,
              <a href="https://www.rogerioferis.org/">Rogerio S Feris</a>,
              <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>,
              <a href="https://people.csail.mit.edu/jrg/">James Glass</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf">paper</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Shvetsova_Everything_at_Once_CVPR_2022_supplemental.pdf">supplement</a> /
              <a href="https://arxiv.org/abs/2112.04446">arXiv</a> /
              <a href="data/bib/shvetsova2022everything.bib">bibtex</a> /
              <a href="https://github.com/ninatu/everything_at_once">code</a>
              <p></p>
              <p>Modality-agnostic self-attention blocks, trained on everything at once  ‚Äì all combinations of modalities, can produce a fused representation of any number of input modalities.</p>
            </td>
          </tr>

        <!--          MOOD 2020: A public Benchmark for Out-of-Distribution Detection and Localization on medical Images                -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/zimmerer2022mood.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9762702" id="MCG_journal">
                <papertitle>MOOD 2020: A public Benchmark for Out-of-Distribution Detection and Localization on medical Images</papertitle>
              </a>
              <br>
              David Zimmerer, Peter M Full, Fabian Isensee, Paul J√§ger, Tim Adler,
              Jens Petersen, Gregor K√∂hler, Tobias Ross, Annika Reinke, Antanas Kascenas,
              Bj√∏rn Sand Jensen, Alison Q O‚ÄôNeil, Jeremy Tan, Benjamin Hou, James Batten,
              Huaqi Qiu, Bernhard Kainz,
              <strong>Nina Shvetsova</strong>, Irina Fedulova, Dmitry V Dylov,
              Baolun Yu, Jianyang Zhai, Jingtao Hu, Runxuan Si, Sihang Zhou, Siqi Wang, Xinyang Li,
              Xuerun Chen, Yang Zhao, Sergio Naval Marimont, Giacomo Tarroni,
              Victor Saase, Lena Maier-Hein, Klaus Maier-Hein
              <br>
              <em>IEEE Transactions on Medical Imaging</em>, 2022
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9762702">paper</a> /
              <a href="data/bib/zimmerer2022mood.bib">bibtex</a> /
              <a href="https://github.com/ninatu/mood_challenge">code of our solution</a>
              <p></p>
              </td>
          </tr>

        <!--          Routing with Self-Attention for Multimodal Capsule Networks                  -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/duarte2021routing.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.00775" id="MCG_journal">
                <papertitle>Routing with Self-Attention for Multimodal Capsule Networks</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=PxD5DrYAAAAJ&hl=en">Kevin Duarte</a>,
              <a href="https://brian7685.github.io/">Brian Chen</a>,
              <strong>Nina Shvetsova</strong>,
              <a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a>,
              <a href="https://scholar.google.com/citations?user=S34WHG0AAAAJ&hl=en">Samuel Thomas</a>,
              <a href="https://alexander-h-liu.github.io/">Alexander Liu</a>,
              <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>,
              <a href="https://people.csail.mit.edu/jrg/">James Glass</a>,
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=p8gsO3gAAAAJ&view_op=list_works&sortby=pubdate">Mubarak Shah</a>,
              <br>
              <em>arxiv</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2112.00775">arXiv</a> /
              <a href="data/bib/duarte2021routing.bib">bibtex</a>
              <p></p>
              <p>Qualities of capsule architectures is used in the context of multimodal learning to learn similar concepts across different modalities.</p>
            </td>
          </tr>

        <!--          Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders                  -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/shvetsova2021anomaly2.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521238" id="MCG_journal">
                <papertitle>Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders</papertitle>
              </a>
              <br>
              <strong>Nina Shvetsova</strong>,
              <a href="https://www.researchgate.net/profile/Bart-Bakker-2">Bart Bakker</a>,
              <a href="https://scholar.google.com/citations?user=c4ZlKeMAAAAJ&hl=en">Irina Fedulova</a>,
              <a href="https://www.researchgate.net/profile/Heinrich-Schulz">Heirich Schulz</a>,
              <a href="https://scholar.google.com/citations?user=mhhvib8AAAAJ&hl=en">Dmitry V. Dylov</a>
              <br>
              <em>IEEE Access</em>, 2021
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521238">paper</a> /
              <a href="https://arxiv.org/abs/2006.13265">arXiv</a> /
              <a href="data/bib/shvetsova2021anomaly.bib">bibtex</a> /
              <a href="https://github.com/ninatu/anomaly_detection">code</a>
              <p></p>
              <p>We establish a strong baseline in anomaly detection in medical images by extending deep autoencoder with progressive growing training to handle high-resolution, complex images.</p>
            </td>
          </tr>

<!--          PIAD                  -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/images/tuluptceva2019perceptual2.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-41404-7_12" id="MCG_journal">
                <papertitle>Perceptual Image Anomaly Detection</papertitle>
              </a>
              <br>
              <strong>Nina Tuluptceva</strong>,
              <a href="https://www.researchgate.net/profile/Bart-Bakker-2">Bart Bakker</a>,
              <a href="https://scholar.google.com/citations?user=c4ZlKeMAAAAJ&hl=en">Irina Fedulova</a>,
              <a href="https://scholar.google.com/citations?user=ZT_k-wMAAAAJ&hl=en">Anton Konushin</a>
              <br>
              <em>ACPR</em>, 2019
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-41404-7_12">paper</a> /
              <a href="https://arxiv.org/abs/1909.05904">arXiv</a> /
              <a href="data/bib/tuluptceva2019perceptual.bib">bibtex</a> /
              <a href="https://github.com/ninatu/piad">code</a>
              <p></p>
              <p>We present a novel method for image anomaly detection leveraging Generative Adversarial Networks to map an image distribution to a predefined latent distribution and vice versa.</p>
              <p>This paper took IAPR Best Paper Award at <a href="https://www.acpr2019.org/">ACPR‚Äô19</a> </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Service</heading>
              <p>
                <b>Co-organizer:</b> &emsp;<a href="https://sites.google.com/view/iccv-mmfm/">MMFM workshop (ICCV 2023)</a>,&nbsp&nbsp&nbsp&nbsp<a href="https://sites.google.com/view/2nd-mmfm-workshop">MMFM workshop (CVPR 2024)</a></b>.
              </p>
               <p>
                <b>Area Chair:</b> &emsp;WACV 2024,&nbsp&nbsp&nbsp&nbspWACV 2025</b>.
              </p>
               <p>
                <b>Reviewer:</b> &emsp;CVPR (<a href="https://cvpr2022.thecvf.com/outstanding-reviewers/">Outstanding reviewer at CVPR 2022</a>),&nbsp&nbspICCV,&nbsp&nbspECCV,&nbsp&nbspNeurIPS,&nbsp&nbspWACV,&nbsp&nbspGCPR,&nbsp&nbspIEEE TPAMI,&nbsp&nbspIEEE TMI</b>.
              </p>
            </td>
          </tr>
        </tbody></table>

<!--&lt;!&ndash;          video example&ndash;&gt;-->
<!--          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>-->
<!--                <source src="images/nerf_supervision.mp4" type="video/mp4">-->
<!--                Your browser does not support the video tag.-->
<!--                </video></div>-->
<!--                <img src='images/nerf_supervision.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function nerfsuper_start() {-->
<!--                  document.getElementById('nerfsuper_image').style.opacity = "1";-->
<!--                }-->

<!--                function nerfsuper_stop() {-->
<!--                  document.getElementById('nerfsuper_image').style.opacity = "0";-->
<!--                }-->
<!--                nerfsuper_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--							<a href="https://waymo.com/research/block-nerf/">-->
<!--                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, -->
<!--              <a href="http://www.peteflorence.com/">Pete Florence</a>, -->
<!--              <strong>Jonathan T. Barron</strong>,  <br>-->
<!--              <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, -->
<!--              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,-->
<!--              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>-->
<!--              <br>-->
<!--              <em>ICRA</em>, 2022  -->
<!--              <br>-->
<!--							<a href="http://yenchenlin.me/nerf-supervision/">project page</a> / -->
<!--							<a href="https://arxiv.org/abs/2203.01913">arXiv</a> / -->
<!--							<a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /-->
<!--							<a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / -->
<!--							<a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				-->
<!--              <p></p>-->
<!--              <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--            &lt;!&ndash;   image example&ndash;&gt;-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/PABMM2015.jpg" alt="PontTuset" width="160" style="border-style: none">-->
<!--            </td>-->
<!--            <td width="75%" valign="middle">-->
<!--              <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal">-->
<!--                <papertitle>Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>-->
<!--              <br>-->
<!--              <em>TPAMI</em>, 2017-->
<!--              <br>-->
<!--              <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /-->
<!--              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /-->
<!--              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a>-->
<!--              <p></p>-->
<!--              <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p>-->
<!--              <p>This paper subsumes our CVPR 2014 paper.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          &lt;!&ndash;   two images example&ndash;&gt;-->
<!--          <tr onmouseout="ddp_stop()" onmouseover="ddp_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='ddp_image'>-->
<!--                  <img src='images/ddp_after.jpg' width="160"></div>-->
<!--                <img src='images/ddp_before.jpg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function ddp_start() {-->
<!--                  document.getElementById('ddp_image').style.opacity = "1";-->
<!--                }-->

<!--                function ddp_stop() {-->
<!--                  document.getElementById('ddp_image').style.opacity = "0";-->
<!--                }-->
<!--                ddp_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/2112.03288">-->
<!--                <papertitle>Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--                    <a href="https://niessnerlab.org/members/barbara_roessle/profile.html">Barbara Roessle</a>,-->
<!--                    <strong>Jonathan T. Barron</strong>,-->
<!--                    <a href="https://bmild.github.io/">Ben Mildenhall</a>,-->
<!--                    <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,-->
<!--                    <a href="https://www.niessnerlab.org/">Matthias Nie√üner</a>-->
<!--              <br>-->
<!--                    <em>CVPR</em>, 2022-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/2112.03288">arXiv</a>-->
<!--              /-->
<!--              <a href="https://www.youtube.com/watch?v=zzkvvdcvksc">video</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Dense depth completion techniques applied to freely-available sparse stereo data can improve NeRF reconstructions in low-data regimes.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
	

<!--        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>-->
<!--          <tr>-->
<!--            <td>-->
<!--              <heading>Misc</heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="20"><tbody>-->

<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>-->
<!--              <br>-->
<!--              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>-->
<!--              <br>-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/cs188.jpg" alt="cs188">-->
<!--            </td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>-->
<!--              <br>-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>-->
<!--              <br>-->
<!--              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--					-->

<!--          <tr>-->
<!--            <td align="center" style="padding:20px;width:25%;vertical-align:middle">-->
<!--							<heading>Basically <br> Blog Posts</heading>-->
<!--            </td>-->
<!--            <td width="75%" valign="middle">-->
<!--              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>-->
<!--            </td>-->
<!--          </tr>-->
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source code and design are borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>,
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
